{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box(obj):\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find('name').text == \"with_mask\":\n",
    "        return 1\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def generate_target(image_id, file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        # Annotation is in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(\"images/\")))\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = 'maksssksksss'+ str(idx) + '.png'\n",
    "        file_label = 'maksssksksss'+ str(idx) + '.xml'\n",
    "        img_path = os.path.join(\"images/\", file_image)\n",
    "        label_path = os.path.join(\"annotations/\", file_label)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        #Generate Label\n",
    "        target = generate_target(idx, label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = MaskDataset(data_transform)\n",
    "train_size = int(len(dataset)*0.8 - (len(dataset)*0.8)%4)\n",
    "test_size = len(dataset)-train_size\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=4, collate_fn=collate_fn)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=4, collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_image(img_tensor, annotation,pred,patch_idx):\n",
    "    fig, axs = plt.subplots(len(img_tensor)//2,len(img_tensor)//2+len(img_tensor)%2,squeeze=True)\n",
    "    for i in range(len(img_tensor)):\n",
    "        ax = axs[i//2,i%2]\n",
    "        img = img_tensor[i].data\n",
    "        ax.axis(\"off\")\n",
    "        # Display the image\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        for j, (box) in enumerate(annotation[i][\"boxes\"]):\n",
    "            xmin, ymin, xmax, ymax = box.detach()\n",
    "            # Create a Rectangle patch\n",
    "            if annotation[i][\"labels\"][j].detach() == 0:\n",
    "                rect = patches.Rectangle(\n",
    "                    (xmin, ymin),\n",
    "                    (xmax - xmin),\n",
    "                    (ymax - ymin),\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"red\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "            elif annotation[i][\"labels\"][j].detach() == 1:\n",
    "                rect = patches.Rectangle(\n",
    "                    (xmin, ymin),\n",
    "                    (xmax - xmin),\n",
    "                    (ymax - ymin),\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"green\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "            elif annotation[i][\"labels\"][j].detach() == 2:\n",
    "                rect = patches.Rectangle(\n",
    "                    (xmin, ymin),\n",
    "                    (xmax - xmin),\n",
    "                    (ymax - ymin),\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"yellow\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "            # Add the patch to the Axes\n",
    "            ax.add_patch(rect)\n",
    "    if not pred:\n",
    "        image_name = \"pre_img\" + patch_idx + \".png\"\n",
    "    else:\n",
    "        image_name = \"img\" + patch_idx + \".png\"\n",
    "    fig.savefig(image_name)\n",
    "\n",
    "\n",
    "def intersection_over_union(box_a, box_b):\n",
    "    # print(box_a,box_b)\n",
    "    # Determine the coordinates of each of the two boxes\n",
    "    xA = max(box_a[0], box_b[0])\n",
    "    yA = max(box_a[1], box_b[1])\n",
    "    xB = min(box_a[2], box_b[2])\n",
    "    yB = min(box_a[3], box_b[3])\n",
    "\n",
    "    # Calculate the area of the intersection area\n",
    "    area_of_intersection = (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "    # Calculate the area of both rectangles\n",
    "    box_a_area = (box_a[2] - box_a[0] + 1) * (box_a[3] - box_a[1] + 1)\n",
    "    box_b_area = (box_b[2] - box_b[0] + 1) * (box_b[3] - box_b[1] + 1)\n",
    "    # Calculate the area of intersection divided by the area of union\n",
    "    # Area of union = sum both areas less the area of intersection\n",
    "    iou = area_of_intersection / float(box_a_area + box_b_area - area_of_intersection)\n",
    "    # Return the score\n",
    "    # if iou < 0:\n",
    "    #     print('warning')\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = list(sorted(os.listdir(\"images/\")))\n",
    "labels = list(sorted(os.listdir(\"annotations/\")))\n",
    "\n",
    "model = get_model_instance_segmentation(3)\n",
    "model.load_state_dict(torch.load('model.pt',map_location='cpu'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m imgs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m imgs)\n\u001b[1;32m      8\u001b[0m annotations \u001b[39m=\u001b[39m [{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m annotations]\n\u001b[0;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(imgs\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     10\u001b[0m pred \u001b[39m=\u001b[39m model(imgs)\n\u001b[1;32m     11\u001b[0m mAP\u001b[39m.\u001b[39mupdate(pred,annotations)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "mAP_list = []\n",
    "score_list = []\n",
    "mAP = MeanAveragePrecision()\n",
    "for imgs, annotations in val_data_loader:\n",
    "    # each patch\n",
    "    imgs = list(img for img in imgs)\n",
    "    annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "    pred = model(imgs)\n",
    "    mAP.update(pred,annotations)\n",
    "    mAP_list.append(mAP.compute()['map'])\n",
    "    # print(pred)\n",
    "    # print(annotations)\n",
    "\n",
    "#     # xmin, ymin, xmax, ymax\n",
    "#     for i in range(len(imgs)):\n",
    "#         score_list.extend(pred[i]['scores'])\n",
    "#         if any(pred[i]['scores']) < 0.7:\n",
    "#             os.makedirs('val_result/wrong/'+str(i),exist_ok=True)\n",
    "#             plot_image(imgs,pred,i,'val_result/wrong/'+str(i))\n",
    "#         # each instance\n",
    "#         bbox = sorted(annotations[i]['boxes'].numpy(),key=lambda x: x[0])\n",
    "#         pred_bbox = sorted(pred[i]['boxes'].detach().numpy(),key=lambda x: x[0])\n",
    "#         iou = 0\n",
    "#         if len(bbox) == len(pred_bbox):\n",
    "#             for i in range(len(bbox)):\n",
    "#                 iou += intersection_over_union(bbox[i],pred_bbox[i])\n",
    "#             acc_list.append(max(iou/len(bbox),0))\n",
    "#         else:\n",
    "#             count = 0\n",
    "#             for i in range(len(pred_bbox)):\n",
    "#                 tmp_iou_list = []\n",
    "#                 for j in range(len(bbox)):\n",
    "#                     tmp_iou_list.append(intersection_over_union(bbox[j],pred_bbox[i]))\n",
    "#                 iou += max(tmp_iou_list)\n",
    "#                 count += 1\n",
    "#             try:\n",
    "#                 acc_list.append(max(iou/count,0))\n",
    "#             except:\n",
    "#                 acc_list.append(max(iou/(count+1),0))\n",
    "#     #     print('-'*100)\n",
    "#     # print(acc_list,end='\\t')\n",
    "#     print(sum(acc_list)/len(acc_list),end='\\t')\n",
    "# # plot_image(imgs[4],pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2342)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(torch.stack(mAP_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q1/1tqmfyd524l_sn2xn11xpfy40000gn/T/ipykernel_42520/401562871.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axs = plt.subplots(len(img_tensor)//2,len(img_tensor)//2+len(img_tensor)%2,squeeze=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,(imgs, annotations) in enumerate(train_data_loader):\n",
    "    \n",
    "    # if idx == 3:\n",
    "    #     break\n",
    "\n",
    "    imgs = list(img for img in imgs)\n",
    "    annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "    pred = model(imgs)\n",
    "    plot_image(imgs, pred,True,str(idx))\n",
    "    plot_image(imgs,annotations,False,str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find annotations number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddd80f1155ab6ac1b3603eec8a4f7af7e571d6991b6f6e5941db98bef78140f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
